{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fed4c7",
   "metadata": {},
   "source": [
    "# Dementia Prediction - Data Preprocessing\n",
    "\n",
    "This notebook handles the initial data loading and dropping of medical/non-feature columns to improve performance and comply with data privacy requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c43f3d0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c528f6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09d331",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9229e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Original shape: (195196, 1024)\n",
      "Columns: ['NACCID', 'NACCADC', 'PACKET', 'FORMVER', 'VISITMO', 'VISITDAY', 'VISITYR', 'NACCVNUM', 'NACCAVST', 'NACCNVST']...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Dataset/Dementia Prediction Dataset.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()[:10]}...\")  # Show first 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302ccf7",
   "metadata": {},
   "source": [
    "## Define Columns to Drop\n",
    "\n",
    "Based on the requirements:\n",
    "- **Non-features**: Administrative IDs, visit tracking, drug columns, and demographic duplicates\n",
    "- **Medical columns**: Cognitive test scores, neurological assessments, pathology data that require medical expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f4ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-feature columns to drop: 53\n",
      "Medical columns to drop: 201\n",
      "Total columns to drop: 254\n"
     ]
    }
   ],
   "source": [
    "# Non-feature columns (administrative, tracking, etc.)\n",
    "drop_cols = ['NACCID', 'NACCADC', 'PACKET', 'VISITMO', 'VISITDAY', 'NACCVNUM', 'NACCAVST', 'NACCNVST', \n",
    "             'NACCDAYS', 'NACCFDYS'] + [f'DRUG{i}' for i in range(1, 41)] + ['HISPORX', 'RACEX', 'PRIMLANX']\n",
    "\n",
    "# Medical columns (cognitive tests, neurological assessments, pathology data)\n",
    "medical_cols = [\n",
    "    'NACCMMSE', 'CDRSUM', 'CDRGLOB', 'ANIMALS', 'TRAILA', 'TRAILB', 'TRAILARR', 'TRAILALI', 'TRAILBRR', 'TRAILBLI', \n",
    "    'MOCATOTS', 'NACCBEHF', 'MEMORY', 'ORIENT', 'JUDGMENT', 'COMMUN', 'HOMEHOBB', 'PERSCARE', 'BILLS', 'TAXES', \n",
    "    'SHOPPING', 'GAMES', 'STOVE', 'MEALPREP', 'EVENTS', 'PAYATTN', 'REMDATES', 'TRAVEL', 'DIGIF', 'DIGIFLEN', \n",
    "    'DIGIB', 'DIGIBLEN', 'BOSTON', 'UDSBENTD', 'VEG', 'UDSVERFN', 'UDSVERNF', 'UDSVERLC', 'UDSVERLR', 'UDSVERLN', \n",
    "    'UDSVERTN', 'UDSVERTE', 'UDSVERTI', 'MOCARECN', 'MOCARECC', 'MOCARECR', 'NACCMOCA', 'CRAFTURS', 'DIGFORCT', \n",
    "    'DIGFORSL', 'DIGBACCT', 'DIGBACLS', 'CRAFTDVR', 'CRAFTDRE', 'MINTTOTW', 'MINTSCNG', 'MINTSCNC', 'MINTPCNG', \n",
    "    'MINTPCNC', 'NACCMOCB', 'REY1REC', 'REY1INT', 'REY2REC', 'REY2INT', 'REY3REC', 'REY3INT', 'REY4REC', 'REY4INT', \n",
    "    'REY5REC', 'REY5INT', 'REY6REC', 'REY6INT', 'OTRLA', 'OTRLARR', 'OTRLALI', 'OTRLB', 'OTRLBRR', 'OTRLBLI', \n",
    "    'REYDREC', 'REYDINT', 'REYTCOR', 'REYFPOS', 'VNTTOT', 'VNTPCNC', 'NACCNEUR', 'NPINF1D', 'NPINF1F', 'NPINF2B', \n",
    "    'NPINF2D', 'NPINF2F', 'NPINF3B', 'NPINF3D', 'NPINF3F', 'NPINF4B', 'NPINF4D', 'NPINF', 'NPHEM', 'NPHEMO', \n",
    "    'NPOLD', 'NPOLDO', 'NPPDXA', 'NPPDXB', 'NPPDXC', 'NPPDXD', 'NPPDXE', 'NPPDXF', 'NPPDXG', 'NPPDXH', 'NPPDXI', \n",
    "    'NPPDXJ', 'NPPDXK', 'NPPDXL', 'NPPDXM', 'NPPDXN', 'NPPDXO', 'NPPDXP', 'NPPDXQ', 'NPPDXR', 'NPPDXS', 'NPPDXT', \n",
    "    'NPFIX', 'NPFIXS', 'NPTHAL', 'NACCBRAA', 'NACCNEUR', 'NACCDIFF', 'NACCDIFFA', 'NACCDIFFB', 'NACCDIFFC', \n",
    "    'NACCDIFFD', 'NACCDIFFE', 'NACCBRAN', 'NACCINF', 'NACCINFN', 'NACCINFM', 'NACCINFD', 'NACCINFT', 'NACCINFO', \n",
    "    'NACCABPE', 'NACCABAS', 'NACCANEUR', 'NACCHEM', 'NACCHEMN', 'NACCHEMM', 'NACCHEMD', 'NACCHEMT', 'NACCHEMO', \n",
    "    'NACCVASC', 'NACCVASN', 'NACCVASM', 'NACCVASD', 'NACCVAST', 'NACCVASO', 'NACCMACR', 'NACCMACN', 'NACCMACM', \n",
    "    'NACCMACD', 'NACCMACT', 'NACCMACO', 'NACCMICR', 'NACCMICN', 'NACCMICM', 'NACCMICD', 'NACCMICT', 'NACCMICO', \n",
    "    'NACCARTE', 'NACCARTN', 'NACCARTM', 'NACCARTD', 'NACCARTT', 'NACCARTO', 'NACCABN', 'NACCABNN', 'NACCABNM', \n",
    "    'NACCABND', 'NACCABNT', 'NACCABNO', 'NACCLBD', 'NACCLBDN', 'NACCLBDM', 'NACCLBDD', 'NACCLBDT', 'NACCLBDO', \n",
    "    'NACCPICK', 'NACCLEWY', 'NACCALZP', 'NACCALZN', 'NACCALZM', 'NACCALZD', 'NACCALZT', 'NACCALZO', 'NACCFTD', \n",
    "    'NACCFTDN', 'NACCFTDM', 'NACCFTDD', 'NACCFTDT', 'NACCFTDO', 'NACCOTHP', 'NACCOTHD', 'NACCOTHH', 'NACCOTHE'\n",
    "]\n",
    "\n",
    "# Combine all columns to drop\n",
    "all_drop_cols = drop_cols + medical_cols\n",
    "\n",
    "print(f\"Non-feature columns to drop: {len(drop_cols)}\")\n",
    "print(f\"Medical columns to drop: {len(medical_cols)}\")\n",
    "print(f\"Total columns to drop: {len(all_drop_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280bdb1",
   "metadata": {},
   "source": [
    "## Drop Columns from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9656f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns found in dataset: 183\n",
      "Columns not found in dataset: 71\n",
      "\n",
      "Missing columns (first 10): ['OTRLA', 'OTRLB', 'VNTTOT', 'NPOLDO', 'NPPDXC', 'NPPDXO', 'NPPDXR', 'NPPDXS', 'NPPDXT', 'NPFIXS']\n",
      "\n",
      "============================================================\n",
      "Original dataset shape: (195196, 1024)\n",
      "Cleaned dataset shape: (195196, 842)\n",
      "Columns dropped: 182\n",
      "Remaining columns: 842\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check which columns exist in the dataset\n",
    "existing_drop_cols = [col for col in all_drop_cols if col in df.columns]\n",
    "missing_cols = [col for col in all_drop_cols if col not in df.columns]\n",
    "\n",
    "print(f\"Columns found in dataset: {len(existing_drop_cols)}\")\n",
    "print(f\"Columns not found in dataset: {len(missing_cols)}\")\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"\\nMissing columns (first 10): {missing_cols[:10]}\")\n",
    "\n",
    "# Drop the existing columns\n",
    "df_cleaned = df.drop(columns=existing_drop_cols, errors='ignore')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Cleaned dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Columns dropped: {df.shape[1] - df_cleaned.shape[1]}\")\n",
    "print(f\"Remaining columns: {df_cleaned.shape[1]}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c3c51",
   "metadata": {},
   "source": [
    "## Preview Remaining Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e0fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns in the dataset:\n",
      "['FORMVER', 'VISITYR', 'NACCCORE', 'NACCREAS', 'NACCREFR', 'BIRTHMO', 'BIRTHYR', 'SEX', 'HISPANIC', 'HISPOR', 'RACE', 'RACESEC', 'RACESECX', 'RACETER', 'RACETERX', 'PRIMLANG', 'EDUC', 'MARISTAT', 'NACCLIVS', 'INDEPEND', 'RESIDENC', 'HANDED', 'INBIRMO', 'INBIRYR', 'INSEX', 'NEWINF', 'INHISP', 'INHISPOR', 'INHISPOX', 'NACCNINR', 'INRACE', 'INRACEX', 'INRASEC', 'INRASECX', 'INRATER', 'INRATERX', 'INEDUC', 'INRELTO', 'INRELTOX', 'INKNOWN', 'INLIVWTH', 'INVISITS', 'INCALLS', 'INRELY', 'NACCFAM', 'NACCMOM', 'NACCDAD', 'NACCAM', 'NACCAMX', 'NACCAMS', 'NACCAMSX', 'NACCFM', 'NACCFMX', 'NACCFMS', 'NACCFMSX', 'NACCOM', 'NACCOMX', 'NACCOMS', 'NACCOMSX', 'NACCFADM', 'NACCFFTD', 'ANYMEDS', 'TOBAC30', 'TOBAC100', 'SMOKYRS', 'PACKSPER', 'QUITSMOK', 'ALCOCCAS', 'ALCFREQ', 'CVHATT', 'HATTMULT', 'HATTYEAR', 'CVAFIB', 'CVANGIO', 'CVBYPASS', 'CVPACDEF', 'CVPACE', 'CVCHF', 'CVANGINA', 'CVHVALVE', 'CVOTHR', 'CVOTHRX', 'CBSTROKE', 'STROKMUL', 'NACCSTYR', 'CBTIA', 'TIAMULT', 'NACCTIYR', 'PD', 'PDYR', 'PDOTHR', 'PDOTHRYR', 'SEIZURES', 'NACCTBI', 'TBI', 'TBIBRIEF', 'TRAUMBRF', 'TBIEXTEN', 'TRAUMEXT', 'TBIWOLOS', 'TRAUMCHR', 'TBIYEAR', 'NCOTHR', 'NCOTHRX', 'DIABETES', 'DIABTYPE', 'HYPERTEN', 'HYPERCHO', 'B12DEF', 'THYROID', 'ARTHRIT', 'ARTHTYPE', 'ARTHTYPX', 'ARTHUPEX', 'ARTHLOEX', 'ARTHSPIN', 'ARTHUNK', 'INCONTU', 'INCONTF', 'APNEA', 'RBD', 'INSOMN', 'OTHSLEEP', 'OTHSLEEX', 'ALCOHOL', 'ABUSOTHR', 'ABUSX', 'PTSD', 'BIPOLAR', 'SCHIZ', 'DEP2YRS', 'DEPOTHR', 'ANXIETY', 'OCD', 'NPSYDEV', 'PSYCDIS', 'PSYCDISX', 'HEIGHT', 'WEIGHT', 'BPSYS', 'BPDIAS', 'HRATE', 'VISION', 'VISCORR', 'VISWCORR', 'HEARING', 'HEARAID', 'HEARWAID', 'ABRUPT', 'STEPWISE', 'SOMATIC', 'EMOT', 'HXHYPER', 'HXSTROKE', 'FOCLSYM', 'FOCLSIGN', 'HACHIN', 'CVDCOG', 'STROKCOG', 'CVDIMAG', 'CVDIMAG1', 'CVDIMAG2', 'CVDIMAG3', 'CVDIMAG4', 'CVDIMAGX', 'PDNORMAL', 'SPEECH', 'SPEECHX', 'FACEXP', 'FACEXPX', 'TRESTFAC', 'TRESTFAX', 'TRESTRHD', 'TRESTRHX', 'TRESTLHD', 'TRESTLHX', 'TRESTRFT', 'TRESTRFX', 'TRESTLFT', 'TRESTLFX', 'TRACTRHD', 'TRACTRHX', 'TRACTLHD', 'TRACTLHX', 'RIGDNECK', 'RIGDNEX', 'RIGDUPRT', 'RIGDUPRX', 'RIGDUPLF', 'RIGDUPLX', 'RIGDLORT', 'RIGDLORX', 'RIGDLOLF', 'RIGDLOLX', 'TAPSRT', 'TAPSRTX', 'TAPSLF', 'TAPSLFX', 'HANDMOVR', 'HANDMVRX', 'HANDMOVL', 'HANDMVLX', 'HANDALTR', 'HANDATRX', 'HANDALTL', 'HANDATLX', 'LEGRT', 'LEGRTX', 'LEGLF', 'LEGLFX', 'ARISING', 'ARISINGX', 'POSTURE', 'POSTUREX', 'GAIT', 'GAITX', 'POSSTAB', 'POSSTABX', 'BRADYKIN', 'BRADYKIX', 'COMPORT', 'CDRLANG', 'NPIQINF', 'NPIQINFX', 'DEL', 'DELSEV', 'HALL', 'HALLSEV', 'AGIT', 'AGITSEV', 'DEPD', 'DEPDSEV', 'ANX', 'ANXSEV', 'ELAT', 'ELATSEV', 'APA', 'APASEV', 'DISN', 'DISNSEV', 'IRR', 'IRRSEV', 'MOT', 'MOTSEV', 'NITE', 'NITESEV', 'APP', 'APPSEV', 'NOGDS', 'SATIS', 'DROPACT', 'EMPTY', 'BORED', 'SPIRITS', 'AFRAID', 'HAPPY', 'HELPLESS', 'STAYHOME', 'MEMPROB', 'WONDRFUL', 'WRTHLESS', 'ENERGY', 'HOPELESS', 'BETTER', 'NACCGDS', 'NACCNREX', 'NORMEXAM', 'FOCLDEF', 'GAITDIS', 'EYEMOVE', 'PARKSIGN', 'RESTTRL', 'RESTTRR', 'SLOWINGL', 'SLOWINGR', 'RIGIDL', 'RIGIDR', 'BRADY', 'PARKGAIT', 'POSTINST', 'CVDSIGNS', 'CORTDEF', 'SIVDFIND', 'CVDMOTL', 'CVDMOTR', 'CORTVISL', 'CORTVISR', 'SOMATL', 'SOMATR', 'POSTCORT', 'PSPCBS', 'EYEPSP', 'DYSPSP', 'AXIALPSP', 'GAITPSP', 'APRAXSP', 'APRAXL', 'APRAXR', 'CORTSENL', 'CORTSENR', 'ATAXL', 'ATAXR', 'ALIENLML', 'ALIENLMR', 'DYSTONL', 'DYSTONR', 'MYOCLLT', 'MYOCLRT', 'ALSFIND', 'GAITNPH', 'OTHNEUR', 'OTHNEURX', 'B9CHG', 'DECSUB', 'DECIN', 'DECCLIN', 'DECCLCOG', 'COGMEM', 'COGORI', 'COGJUDG', 'COGLANG', 'COGVIS', 'COGATTN', 'COGFLUC', 'COGFLAGO', 'COGOTHR', 'COGOTHRX', 'NACCCOGF', 'NACCCGFX', 'COGMODE', 'COGMODEX', 'DECAGE', 'DECCLBE', 'BEAPATHY', 'BEDEP', 'BEVHALL', 'BEVWELL', 'BEVHAGO', 'BEAHALL', 'BEDEL', 'BEDISIN', 'BEIRRIT', 'BEAGIT', 'BEPERCH', 'BEREM', 'BEREMAGO', 'BEANX', 'BEOTHR', 'BEOTHRX', 'NACCBEFX', 'BEMODE', 'BEMODEX', 'BEAGE', 'DECCLMOT', 'MOGAIT', 'MOFALLS', 'MOTREM', 'MOSLOW', 'NACCMOTF', 'MOMODE', 'MOMODEX', 'MOMOPARK', 'PARKAGE', 'MOMOALS', 'ALSAGE', 'MOAGE', 'COURSE', 'FRSTCHG', 'LBDEVAL', 'FTLDEVAL', 'MMSECOMP', 'MMSELOC', 'MMSELAN', 'MMSELANX', 'MMSEVIS', 'MMSEHEAR', 'MMSEORDA', 'MMSEORLO', 'PENTAGON', 'NPSYCLOC', 'NPSYLAN', 'NPSYLANX', 'LOGIMO', 'LOGIDAY', 'LOGIYR', 'LOGIPREV', 'LOGIMEM', 'MEMUNITS', 'MEMTIME', 'UDSBENTC', 'UDSBENRS', 'WAIS', 'UDSVERFC', 'COGSTAT', 'NACCC1', 'MOCACOMP', 'MOCAREAS', 'MOCALOC', 'MOCALAN', 'MOCALANX', 'MOCAVIS', 'MOCAHEAR', 'MOCATRAI', 'MOCACUBE', 'MOCACLOC', 'MOCACLON', 'MOCACLOH', 'MOCANAMI', 'MOCAREGI', 'MOCADIGI', 'MOCALETT', 'MOCASER7', 'MOCAREPE', 'MOCAFLUE', 'MOCAABST', 'MOCAORDT', 'MOCAORMO', 'MOCAORYR', 'MOCAORDY', 'MOCAORPL', 'MOCAORCT', 'CRAFTVRS', 'CRAFTDTI', 'CRAFTCUE', 'MINTTOTS', 'NACCC2', 'MODCOMM', 'MOCBTOTS', 'OTRAILA', 'OTRAILB', 'VNTTOTW', 'RESPVAL', 'RESPHEAR', 'RESPDIST', 'RESPINTR', 'RESPDISN', 'RESPFATG', 'RESPEMOT', 'RESPASST', 'RESPOTH', 'RESPOTHX', 'WHODIDDX', 'DXMETHOD', 'NORMCOG', 'DEMENTED', 'AMNDEM', 'PCA', 'NACCPPA', 'NACCPPAG', 'NACCPPME', 'NACCBVFT', 'NACCLBDS', 'NAMNDEM', 'NACCTMCI', 'NACCMCIL', 'NACCMCIA', 'NACCMCIE', 'NACCMCIV', 'NACCMCII', 'IMPNOMCI', 'AMYLPET', 'AMYLCSF', 'FDGAD', 'HIPPATR', 'TAUPETAD', 'CSFTAU', 'FDGFTLD', 'TPETFTLD', 'MRFTLD', 'DATSCAN', 'OTHBIOM', 'OTHBIOMX', 'IMAGLINF', 'IMAGLAC', 'IMAGMACH', 'IMAGMICH', 'IMAGMWMH', 'IMAGEWMH', 'OTHMUT', 'OTHMUTX', 'PROBAD', 'PROBADIF', 'POSSAD', 'POSSADIF', 'NACCLBDE', 'NACCLBDP', 'PARK', 'MSA', 'MSAIF', 'PSP', 'PSPIF', 'CORT', 'CORTIF', 'FTLDMO', 'FTLDMOIF', 'FTLDNOS', 'FTLDNOIF', 'FTD', 'FTDIF', 'PPAPH', 'PPAPHIF', 'FTLDSUBT', 'FTLDSUBX', 'CVD', 'CVDIF', 'PREVSTK', 'STROKDEC', 'STKIMAG', 'INFNETW', 'INFWMH', 'VASC', 'VASCIF', 'VASCPS', 'VASCPSIF', 'STROKE', 'STROKIF', 'ESSTREM', 'ESSTREIF', 'DOWNS', 'DOWNSIF', 'HUNT', 'HUNTIF', 'PRION', 'PRIONIF', 'BRNINJ', 'BRNINJIF', 'BRNINCTE', 'HYCEPH', 'HYCEPHIF', 'EPILEP', 'EPILEPIF', 'NEOP', 'NEOPIF', 'NEOPSTAT', 'HIV', 'HIVIF', 'OTHCOG', 'OTHCOGIF', 'OTHCOGX', 'DEP', 'DEPIF', 'DEPTREAT', 'BIPOLDX', 'BIPOLDIF', 'SCHIZOP', 'SCHIZOIF', 'ANXIET', 'ANXIETIF', 'DELIR', 'DELIRIF', 'PTSDDX', 'PTSDDXIF', 'OTHPSY', 'OTHPSYIF', 'OTHPSYX', 'ALCDEM', 'ALCDEMIF', 'ALCABUSE', 'IMPSUB', 'IMPSUBIF', 'DYSILL', 'DYSILLIF', 'MEDS', 'MEDSIF', 'DEMUN', 'DEMUNIF', 'COGOTH', 'COGOTHIF', 'COGOTHX', 'COGOTH2', 'COGOTH2F', 'COGOTH2X', 'COGOTH3', 'COGOTH3F', 'COGOTH3X', 'NACCETPR', 'NACCADMU', 'CANCER', 'CANCSITE', 'DIABET', 'MYOINF', 'CONGHRT', 'AFIBRILL', 'HYPERT', 'ANGINA', 'HYPCHOL', 'VB12DEF', 'THYDIS', 'ARTH', 'ARTYPE', 'ARTYPEX', 'ARTUPEX', 'ARTLOEX', 'ARTSPIN', 'ARTUNKN', 'URINEINC', 'BOWLINC', 'SLEEPAP', 'REMDIS', 'HYPOSOM', 'SLEEPOTH', 'SLEEPOTX', 'ANGIOCP', 'ANGIOPCI', 'PACEMAKE', 'HVALVE', 'ANTIENC', 'ANTIENCX', 'OTHCOND', 'OTHCONDX', 'NACCAGEB', 'NACCNIHR', 'NACCNORM', 'NACCIDEM', 'NACCAGE', 'NACCAAAS', 'NACCAANX', 'NACCAC', 'NACCACEI', 'NACCADEP', 'NACCADMD', 'NACCAHTN', 'NACCAMD', 'NACCANGI', 'NACCAPSY', 'NACCBETA', 'NACCCCBS', 'NACCDBMD', 'NACCDIUR', 'NACCEMD', 'NACCEPMD', 'NACCHTNC', 'NACCLIPL', 'NACCNSD', 'NACCPDMD', 'NACCBMI', 'NACCUDSD', 'NACCDIED', 'NACCMOD', 'NACCYOD', 'NACCAUTP', 'NACCACTV', 'NACCNOVS', 'NACCDSMO', 'NACCDSDY', 'NACCDSYR', 'NACCNURP', 'NACCNRMO', 'NACCNRDY', 'NACCNRYR', 'NACCMDSS', 'NACCPAFF', 'NACCACSF', 'NACCPCSF', 'NACCTCSF', 'NACCMRSA', 'NACCNMRI', 'NACCAPSA', 'NACCNAPA', 'TELCOV', 'TELMOD', 'ADGCGWAS', 'ADGCEXOM', 'ADGCRND', 'ADGCEXR', 'NGDSGWAS', 'NGDSEXOM', 'NGDSWGS', 'NGDSWES', 'NGDSGWAC', 'NGDSEXAC', 'NGDSWGAC', 'NGDSWEAC', 'NACCNCRD', 'NACCAPOE', 'NACCNE4S', 'NPFORMVER', 'NPSEX', 'NPPMIH', 'NPFIXX', 'NPWBRWT', 'NPWBRF', 'NACCBRNN', 'NPGRCCA', 'NPGRLA', 'NPGRHA', 'NPGRSNH', 'NPGRLCH', 'NACCAVAS', 'NPTAN', 'NPTANX', 'NPABAN', 'NPABANX', 'NPASAN', 'NPASANX', 'NPTDPAN', 'NPTDPANX', 'NPHISMB', 'NPHISG', 'NPHISSS', 'NPHIST', 'NPHISO', 'NPHISOX', 'NPADNC', 'NACCAMY', 'NPLINF', 'NPLAC', 'NPINF1A', 'NPINF1B', 'NPINF2A', 'NPINF3A', 'NPINF4A', 'NPINF4F', 'NPHEMO1', 'NPHEMO2', 'NPHEMO3', 'NPMICRO', 'NPOLD1', 'NPOLD2', 'NPOLD3', 'NPOLD4', 'NPOLDD', 'NPOLDD1', 'NPOLDD2', 'NPOLDD3', 'NPOLDD4', 'NPWMR', 'NPPATH', 'NACCNEC', 'NPPATH2', 'NPPATH3', 'NPPATH4', 'NPPATH5', 'NPPATH6', 'NPPATH7', 'NPPATH8', 'NPPATH9', 'NPPATH10', 'NPPATH11', 'NPPATHO', 'NPPATHOX', 'NPART', 'NPOANG', 'NPLBOD', 'NPNLOSS', 'NPHIPSCL', 'NPSCL', 'NPFTDTAU', 'NPFTDT2', 'NACCCBD', 'NACCPROG', 'NPFTDT5', 'NPFTDT6', 'NPFTDT7', 'NPFTDT8', 'NPFTDT9', 'NPFTDT10', 'NPFRONT', 'NPTAU', 'NPFTD', 'NPFTDTDP', 'NPALSMND', 'NPOFTD', 'NPOFTD1', 'NPOFTD2', 'NPOFTD3', 'NPOFTD4', 'NPOFTD5', 'NPFTDNO', 'NPFTDSPC', 'NPTDPA', 'NPTDPB', 'NPTDPC', 'NPTDPD', 'NPTDPE', 'NACCPRIO', 'NACCDOWN', 'NACCWRI1', 'NACCWRI2', 'NACCWRI3', 'NACCBNKF', 'NPBNKB', 'NACCFORM', 'NACCPARA', 'NACCCSFP', 'NPBNKF', 'NPFAUT', 'NPFAUT1', 'NPFAUT2', 'NPFAUT3', 'NPFAUT4', 'NACCDAGE', 'NACCINT', 'NPNIT', 'NPCERAD', 'NPADRDA', 'NPOCRIT', 'NPVOTH', 'NPLEWYCS', 'NPGENE', 'NPFHSPEC', 'NPTAUHAP', 'NPPRNP', 'NPCHROM', 'NPPNORM', 'NPCNORM', 'NPPADP', 'NPCADP', 'NPPAD', 'NPCAD', 'NPPLEWY', 'NPCLEWY', 'NPPVASC', 'NPCVASC', 'NPPFTLD', 'NPCFTLD', 'NPPHIPP', 'NPCHIPP', 'NPPPRION', 'NPCPRION', 'NPPOTH1', 'NPCOTH1', 'NPOTH1X', 'NPPOTH2', 'NPCOTH2', 'NPOTH2X', 'NPPOTH3', 'NPCOTH3', 'NPOTH3X', 'NPARTAG', 'NPATGSEV', 'NPATGAMY', 'NPATGAM1', 'NPATGAM2', 'NPATGAM3', 'NPATGAM4', 'NPATGAM5', 'NPATGFRN', 'NPATGFR1', 'NPATGFR2', 'NPATGFR3', 'NPATGFR4']\n",
      "\n",
      "First 5 rows of cleaned dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FORMVER</th>\n",
       "      <th>VISITYR</th>\n",
       "      <th>NACCCORE</th>\n",
       "      <th>NACCREAS</th>\n",
       "      <th>NACCREFR</th>\n",
       "      <th>BIRTHMO</th>\n",
       "      <th>BIRTHYR</th>\n",
       "      <th>SEX</th>\n",
       "      <th>HISPANIC</th>\n",
       "      <th>HISPOR</th>\n",
       "      <th>...</th>\n",
       "      <th>NPATGAM1</th>\n",
       "      <th>NPATGAM2</th>\n",
       "      <th>NPATGAM3</th>\n",
       "      <th>NPATGAM4</th>\n",
       "      <th>NPATGAM5</th>\n",
       "      <th>NPATGFRN</th>\n",
       "      <th>NPATGFR1</th>\n",
       "      <th>NPATGFR2</th>\n",
       "      <th>NPATGFR3</th>\n",
       "      <th>NPATGFR4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1952</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1952</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1956</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1958</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 842 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FORMVER  VISITYR  NACCCORE  NACCREAS  NACCREFR  BIRTHMO  BIRTHYR  SEX  \\\n",
       "0      3.0     2022         1         7         2        5     1952    1   \n",
       "1      3.0     2024         1         7         2        5     1952    1   \n",
       "2      3.0     2023         1         7         2       12     1956    1   \n",
       "3      3.0     2021         1         7         1        1     1958    2   \n",
       "4      3.0     2022         1         1         2        2     1945    1   \n",
       "\n",
       "   HISPANIC  HISPOR  ...  NPATGAM1  NPATGAM2 NPATGAM3  NPATGAM4 NPATGAM5  \\\n",
       "0         0      88  ...        -4        -4       -4        -4       -4   \n",
       "1         0      88  ...        -4        -4       -4        -4       -4   \n",
       "2         0      88  ...        -4        -4       -4        -4       -4   \n",
       "3         1       1  ...        -4        -4       -4        -4       -4   \n",
       "4         1       1  ...        -4        -4       -4        -4       -4   \n",
       "\n",
       "   NPATGFRN  NPATGFR1  NPATGFR2  NPATGFR3  NPATGFR4  \n",
       "0        -4        -4        -4        -4        -4  \n",
       "1        -4        -4        -4        -4        -4  \n",
       "2        -4        -4        -4        -4        -4  \n",
       "3        -4        -4        -4        -4        -4  \n",
       "4        -4        -4        -4        -4        -4  \n",
       "\n",
       "[5 rows x 842 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display remaining columns\n",
    "print(\"Remaining columns in the dataset:\")\n",
    "print(df_cleaned.columns.tolist())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of cleaned dataset:\")\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565218a1",
   "metadata": {},
   "source": [
    "## Save Cleaned Dataset\n",
    "\n",
    "Save the cleaned dataset for faster loading in future preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a2119e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully as 'Dataset/Dementia_Cleaned.csv'\n",
      "File size reduced by approximately 17.8% in columns\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "df_cleaned.to_csv('Dataset/Dementia_Cleaned.csv', index=False)\n",
    "\n",
    "print(\"Cleaned dataset saved successfully as 'Dataset/Dementia_Cleaned.csv'\")\n",
    "print(f\"File size reduced by approximately {((df.shape[1] - df_cleaned.shape[1]) / df.shape[1] * 100):.1f}% in columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b6956c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "Now we'll preprocess the cleaned dataset to prepare it for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f31627d",
   "metadata": {},
   "source": [
    "## Step 1: Data Exploration & Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5037c861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET OVERVIEW\n",
      "============================================================\n",
      "Shape: (195196, 842)\n",
      "Rows: 195,196\n",
      "Columns: 842\n",
      "\n",
      "Memory usage: 1720.84 MB\n",
      "\n",
      "============================================================\n",
      "DATA TYPES\n",
      "============================================================\n",
      "int64      628\n",
      "float64    117\n",
      "object      97\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic data exploration\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {df_cleaned.shape}\")\n",
    "print(f\"Rows: {df_cleaned.shape[0]:,}\")\n",
    "print(f\"Columns: {df_cleaned.shape[1]}\")\n",
    "print(f\"\\nMemory usage: {df_cleaned.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*60)\n",
    "print(df_cleaned.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27d78ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "============================================================\n",
      "Total columns with missing values: 201\n",
      "\n",
      "Top 20 columns with most missing values:\n",
      "  Column  Missing_Count  Missing_Percent\n",
      "RACETERX         195160        99.981557\n",
      "TRESTFAX         195157        99.980020\n",
      "ANTIENCX         195144        99.973360\n",
      "INRATERX         195137        99.969774\n",
      "TRESTRHX         195135        99.968749\n",
      "TRESTLHX         195133        99.967725\n",
      " FACEXPX         195127        99.964651\n",
      "TRESTRFX         195125        99.963626\n",
      "TRESTLFX         195121        99.961577\n",
      " NPOTH3X         195088        99.944671\n",
      "COGOTH3X         195044        99.922130\n",
      "FTLDSUBX         195007        99.903174\n",
      " OTHMUTX         194989        99.893953\n",
      "RIGDUPRX         194958        99.878071\n",
      "TRACTRHX         194958        99.878071\n",
      "INHISPOX         194958        99.878071\n",
      "TRACTLHX         194952        99.874997\n",
      "RIGDUPLX         194952        99.874997\n",
      "RIGDLORX         194905        99.850919\n",
      " RIGDNEX         194903        99.849894\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"=\"*60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "missing_counts = df_cleaned.isnull().sum()\n",
    "missing_percent = (missing_counts / len(df_cleaned)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_counts.index,\n",
    "    'Missing_Count': missing_counts.values,\n",
    "    'Missing_Percent': missing_percent.values\n",
    "})\n",
    "\n",
    "# Show columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "print(f\"Total columns with missing values: {len(missing_df)}\")\n",
    "print(f\"\\nTop 20 columns with most missing values:\")\n",
    "print(missing_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2a431aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING FOR SPECIAL MISSING CODES\n",
      "============================================================\n",
      "Value -4 appears 68,232,396 times across all columns\n",
      "Value 88 appears 1,717,857 times across all columns\n",
      "Value 99 appears 52,317 times across all columns\n",
      "Value 999 appears 6,589 times across all columns\n",
      "\n",
      "Sample value counts for first 5 numeric columns:\n",
      "\n",
      "FORMVER:\n",
      "FORMVER\n",
      "3.0    80758\n",
      "2.0    79944\n",
      "1.0    23944\n",
      "3.2    10550\n",
      "Name: count, dtype: int64\n",
      "\n",
      "VISITYR:\n",
      "VISITYR\n",
      "2009    11907\n",
      "2010    11619\n",
      "2008    11526\n",
      "2012    11440\n",
      "2013    11244\n",
      "2019    11015\n",
      "2007    10920\n",
      "2011    10877\n",
      "2014    10831\n",
      "2023    10660\n",
      "Name: count, dtype: int64\n",
      "\n",
      "NACCCORE:\n",
      "NACCCORE\n",
      "1    195196\n",
      "Name: count, dtype: int64\n",
      "\n",
      "NACCREAS:\n",
      "NACCREAS\n",
      "1    150029\n",
      "2     32778\n",
      "7     12138\n",
      "9       251\n",
      "Name: count, dtype: int64\n",
      "\n",
      "NACCREFR:\n",
      "NACCREFR\n",
      "2    70092\n",
      "1    66858\n",
      "8    53132\n",
      "9     5114\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for special missing value codes (common in medical datasets)\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING FOR SPECIAL MISSING CODES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Common codes: -4, -9, 88, 99, 999 often mean \"missing\", \"unknown\", \"not applicable\"\n",
    "special_codes = [-4, -9, 88, 99, 999]\n",
    "\n",
    "for code in special_codes:\n",
    "    count = (df_cleaned == code).sum().sum()\n",
    "    if count > 0:\n",
    "        print(f\"Value {code} appears {count:,} times across all columns\")\n",
    "        \n",
    "# Sample a few numeric columns to see value distributions\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns[:5]\n",
    "print(f\"\\nSample value counts for first 5 numeric columns:\")\n",
    "for col in numeric_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df_cleaned[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c50018f",
   "metadata": {},
   "source": [
    "## Step 2: Identify Target Variable\n",
    "\n",
    "We need to identify which column represents dementia diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5acd1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SEARCHING FOR TARGET VARIABLE\n",
      "============================================================\n",
      "Found 3 potential target columns:\n",
      "  - DEMENTED: 2 unique values\n",
      "    Values: {0: 137606, 1: 57590}\n",
      "  - NACCETPR: 32 unique values\n",
      "  - NORMCOG: 2 unique values\n",
      "    Values: {0: 100263, 1: 94933}\n",
      "\n",
      "============================================================\n",
      "LOW CARDINALITY COLUMNS (2-5 unique values)\n",
      "============================================================\n",
      "Found 519 columns\n",
      "Sample: ['FORMVER', 'NACCREAS', 'NACCREFR', 'SEX', 'HISPANIC', 'INDEPEND', 'RESIDENC', 'HANDED', 'INSEX', 'NEWINF']\n"
     ]
    }
   ],
   "source": [
    "# Search for likely target variables (dementia, diagnosis, etc.)\n",
    "target_keywords = ['DEMENTIA', 'DEMENTED', 'DIAGNOSIS', 'NACCALZD', 'NACCETPR', 'NORMCOG']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SEARCHING FOR TARGET VARIABLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "potential_targets = []\n",
    "for col in df_cleaned.columns:\n",
    "    for keyword in target_keywords:\n",
    "        if keyword in col.upper():\n",
    "            potential_targets.append(col)\n",
    "            \n",
    "potential_targets = list(set(potential_targets))\n",
    "\n",
    "print(f\"Found {len(potential_targets)} potential target columns:\")\n",
    "for col in potential_targets[:20]:  # Show first 20\n",
    "    print(f\"  - {col}: {df_cleaned[col].nunique()} unique values\")\n",
    "    if df_cleaned[col].nunique() < 10:\n",
    "        print(f\"    Values: {df_cleaned[col].value_counts().to_dict()}\")\n",
    "        \n",
    "# Let's also check columns with low cardinality (likely categorical/target)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LOW CARDINALITY COLUMNS (2-5 unique values)\")\n",
    "print(\"=\"*60)\n",
    "low_card = [col for col in df_cleaned.columns if 2 <= df_cleaned[col].nunique() <= 5]\n",
    "print(f\"Found {len(low_card)} columns\")\n",
    "print(f\"Sample: {low_card[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a288a41",
   "metadata": {},
   "source": [
    "## Step 3: Handle Special Missing Value Codes\n",
    "\n",
    "Replace special codes (-4, -9, 88, 99, etc.) with NaN for proper handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cfc7d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPECIAL MISSING CODE REPLACEMENT\n",
      "============================================================\n",
      "Missing values before: 17,889,195\n",
      "Missing values after: 87,898,354\n",
      "New missing values created: 70,009,159\n",
      "\n",
      "Shape: (195196, 842)\n"
     ]
    }
   ],
   "source": [
    "# Replace special missing codes with NaN\n",
    "# Common codes in medical datasets: -4, -9, 88, 99, 999\n",
    "special_missing_codes = [-4, -9, 88, 99, 999]\n",
    "\n",
    "df_processed = df_cleaned.copy()\n",
    "\n",
    "# Count before replacement\n",
    "before_count = df_processed.isnull().sum().sum()\n",
    "\n",
    "# Replace special codes with NaN\n",
    "for code in special_missing_codes:\n",
    "    df_processed = df_processed.replace(code, np.nan)\n",
    "\n",
    "# Count after replacement\n",
    "after_count = df_processed.isnull().sum().sum()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPECIAL MISSING CODE REPLACEMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Missing values before: {before_count:,}\")\n",
    "print(f\"Missing values after: {after_count:,}\")\n",
    "print(f\"New missing values created: {after_count - before_count:,}\")\n",
    "print(f\"\\nShape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c032ba6",
   "metadata": {},
   "source": [
    "## Step 4: Drop High Missing Value Columns\n",
    "\n",
    "Remove columns with >70% missing data as they won't be useful for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5873f1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DROPPING COLUMNS WITH >70.0% MISSING VALUES\n",
      "============================================================\n",
      "Columns to drop: 303\n",
      "Sample columns: ['HISPOR', 'RACESEC', 'RACESECX', 'RACETER', 'RACETERX', 'INHISPOR', 'INHISPOX', 'INRACEX', 'INRASEC', 'INRASECX']\n",
      "\n",
      "Shape after dropping high-missing columns: (195196, 539)\n",
      "Columns remaining: 539\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with >70% missing values\n",
    "missing_threshold = 0.70\n",
    "\n",
    "missing_percent = df_processed.isnull().sum() / len(df_processed)\n",
    "high_missing_cols = missing_percent[missing_percent > missing_threshold].index.tolist()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"DROPPING COLUMNS WITH >{missing_threshold*100}% MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Columns to drop: {len(high_missing_cols)}\")\n",
    "if len(high_missing_cols) > 0:\n",
    "    print(f\"Sample columns: {high_missing_cols[:10]}\")\n",
    "\n",
    "df_processed = df_processed.drop(columns=high_missing_cols)\n",
    "\n",
    "print(f\"\\nShape after dropping high-missing columns: {df_processed.shape}\")\n",
    "print(f\"Columns remaining: {df_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc65f29",
   "metadata": {},
   "source": [
    "## Step 5: Feature Engineering - Create Age Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98acb1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AGE FEATURE CREATED\n",
      "============================================================\n",
      "Age statistics:\n",
      "count    195196.000000\n",
      "mean         74.841047\n",
      "std          10.305351\n",
      "min          18.000000\n",
      "25%          69.000000\n",
      "50%          75.000000\n",
      "75%          82.000000\n",
      "max         111.000000\n",
      "Name: AGE, dtype: float64\n",
      "\n",
      "Unrealistic ages (< 0 or > 120): 0\n",
      "Dropped birth-related columns: ['BIRTHYR', 'BIRTHMO']\n"
     ]
    }
   ],
   "source": [
    "# Create age feature from birth year and visit year\n",
    "if 'BIRTHYR' in df_processed.columns and 'VISITYR' in df_processed.columns:\n",
    "    df_processed['AGE'] = df_processed['VISITYR'] - df_processed['BIRTHYR']\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"AGE FEATURE CREATED\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Age statistics:\")\n",
    "    print(df_processed['AGE'].describe())\n",
    "    \n",
    "    # Check for unrealistic ages\n",
    "    unrealistic = df_processed[(df_processed['AGE'] < 0) | (df_processed['AGE'] > 120)]\n",
    "    print(f\"\\nUnrealistic ages (< 0 or > 120): {len(unrealistic)}\")\n",
    "    \n",
    "    # Drop BIRTHYR and BIRTHMO as we now have AGE\n",
    "    cols_to_drop = [col for col in ['BIRTHYR', 'BIRTHMO'] if col in df_processed.columns]\n",
    "    if cols_to_drop:\n",
    "        df_processed = df_processed.drop(columns=cols_to_drop)\n",
    "        print(f\"Dropped birth-related columns: {cols_to_drop}\")\n",
    "else:\n",
    "    print(\"BIRTHYR or VISITYR not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2b62a",
   "metadata": {},
   "source": [
    "## Step 6: Separate Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da1fcc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE TYPE IDENTIFICATION\n",
      "============================================================\n",
      "Numerical columns: 531\n",
      "Categorical columns: 7\n",
      "\n",
      "Sample numerical columns: ['FORMVER', 'VISITYR', 'NACCCORE', 'NACCREAS', 'NACCREFR', 'SEX', 'HISPANIC', 'RACE', 'PRIMLANG', 'EDUC']\n",
      "Sample categorical columns: ['RESPOTHX', 'ADGCRND', 'ADGCEXR', 'NGDSGWAC', 'NGDSEXAC', 'NGDSWGAC', 'NGDSWEAC']\n",
      "\n",
      "============================================================\n",
      "CHECKING NUMERICAL COLUMN CARDINALITY\n",
      "============================================================\n",
      "Numerical columns with ≤20 unique values (likely categorical): 489\n",
      "Sample:\n",
      "  FORMVER: 4 unique values\n",
      "  VISITYR: 20 unique values\n",
      "  NACCCORE: 1 unique values\n",
      "  NACCREAS: 4 unique values\n",
      "  NACCREFR: 4 unique values\n",
      "  SEX: 2 unique values\n",
      "  HISPANIC: 3 unique values\n",
      "  RACE: 6 unique values\n",
      "  PRIMLANG: 8 unique values\n",
      "  MARISTAT: 7 unique values\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE TYPE IDENTIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Numerical columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "print(f\"\\nSample numerical columns: {numerical_cols[:10]}\")\n",
    "print(f\"Sample categorical columns: {categorical_cols[:10]}\")\n",
    "\n",
    "# Check cardinality of numerical columns (some might be categorical)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING NUMERICAL COLUMN CARDINALITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "low_cardinality_numeric = []\n",
    "for col in numerical_cols:\n",
    "    unique_count = df_processed[col].nunique()\n",
    "    if unique_count <= 20:  # Likely categorical\n",
    "        low_cardinality_numeric.append((col, unique_count))\n",
    "\n",
    "print(f\"Numerical columns with ≤20 unique values (likely categorical): {len(low_cardinality_numeric)}\")\n",
    "if low_cardinality_numeric:\n",
    "    print(\"Sample:\")\n",
    "    for col, count in low_cardinality_numeric[:10]:\n",
    "        print(f\"  {col}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10978f",
   "metadata": {},
   "source": [
    "## Step 7: Handle Missing Values - Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97f75fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUE IMPUTATION\n",
      "============================================================\n",
      "Imputed 531 numerical columns with median\n",
      "Imputed 7 categorical columns with most frequent value\n",
      "\n",
      "Missing values after imputation: 0\n",
      "Shape: (195196, 538)\n"
     ]
    }
   ],
   "source": [
    "# Imputation strategy\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MISSING VALUE IMPUTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For numerical: use median (robust to outliers)\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# For categorical: use most frequent\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply imputation\n",
    "if len(numerical_cols) > 0:\n",
    "    df_processed[numerical_cols] = numerical_imputer.fit_transform(df_processed[numerical_cols])\n",
    "    print(f\"Imputed {len(numerical_cols)} numerical columns with median\")\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    df_processed[categorical_cols] = categorical_imputer.fit_transform(df_processed[categorical_cols])\n",
    "    print(f\"Imputed {len(categorical_cols)} categorical columns with most frequent value\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "remaining_missing = df_processed.isnull().sum().sum()\n",
    "print(f\"\\nMissing values after imputation: {remaining_missing}\")\n",
    "print(f\"Shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9e54e",
   "metadata": {},
   "source": [
    "## Step 8: Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a51e2fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CATEGORICAL ENCODING\n",
      "============================================================\n",
      "Encoding 7 categorical columns\n",
      "  RESPOTHX: 227 classes\n",
      "  ADGCRND: 55 classes\n",
      "  ADGCEXR: 7 classes\n",
      "  NGDSGWAC: 8 classes\n",
      "  NGDSEXAC: 4 classes\n",
      "  NGDSWGAC: 2 classes\n",
      "  NGDSWEAC: 2 classes\n",
      "\n",
      "All categorical columns encoded successfully\n",
      "\n",
      "Final shape: (195196, 538)\n",
      "All columns are now numerical: True\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CATEGORICAL ENCODING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Refresh categorical columns list\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"Encoding {len(categorical_cols)} categorical columns\")\n",
    "    \n",
    "    le_dict = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "        le_dict[col] = le\n",
    "        print(f\"  {col}: {len(le.classes_)} classes\")\n",
    "    \n",
    "    print(f\"\\nAll categorical columns encoded successfully\")\n",
    "else:\n",
    "    print(\"No categorical columns to encode\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df_processed.shape}\")\n",
    "print(f\"All columns are now numerical: {df_processed.select_dtypes(include=[np.number]).shape[1] == df_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb424b2",
   "metadata": {},
   "source": [
    "## Step 9: Save Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae34c160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPROCESSING COMPLETE!\n",
      "============================================================\n",
      "Preprocessed dataset saved: 'Dataset/Dementia_Preprocessed.csv'\n",
      "\n",
      "Final dataset summary:\n",
      "  Shape: (195196, 538)\n",
      "  Rows: 195,196\n",
      "  Features: 538\n",
      "  Missing values: 0\n",
      "  Memory: 801.20 MB\n",
      "\n",
      "✅ Ready for train-test split and model training!\n"
     ]
    }
   ],
   "source": [
    "# Save the preprocessed dataset\n",
    "df_processed.to_csv('Dataset/Dementia_Preprocessed.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Preprocessed dataset saved: 'Dataset/Dementia_Preprocessed.csv'\")\n",
    "print(f\"\\nFinal dataset summary:\")\n",
    "print(f\"  Shape: {df_processed.shape}\")\n",
    "print(f\"  Rows: {df_processed.shape[0]:,}\")\n",
    "print(f\"  Features: {df_processed.shape[1]}\")\n",
    "print(f\"  Missing values: {df_processed.isnull().sum().sum()}\")\n",
    "print(f\"  Memory: {df_processed.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\n✅ Ready for train-test split and model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf85fae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary of Preprocessing Steps\n",
    "\n",
    "✅ **Completed:**\n",
    "1. **Data Exploration** - Analyzed dataset structure, missing values, and special codes\n",
    "2. **Target Variable Identification** - Searched for dementia diagnosis columns\n",
    "3. **Special Code Handling** - Replaced -4, -9, 88, 99, 999 with NaN\n",
    "4. **Column Dropping** - Removed columns with >70% missing values\n",
    "5. **Feature Engineering** - Created AGE feature from birth year\n",
    "6. **Feature Separation** - Identified numerical vs categorical features\n",
    "7. **Missing Value Imputation** - Median for numerical, mode for categorical\n",
    "8. **Categorical Encoding** - Label encoded all categorical variables\n",
    "9. **Data Saved** - Preprocessed dataset ready for modeling\n",
    "\n",
    "**Next Steps:**\n",
    "- Identify and separate target variable\n",
    "- Train-test split (stratified)\n",
    "- Feature scaling (if needed)\n",
    "- Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305150f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Feature Selection & Reduction\n",
    "\n",
    "With 538 features, we need to reduce dimensionality before class balancing to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b580dee",
   "metadata": {},
   "source": [
    "## Step A: Remove Low Variance Features\n",
    "\n",
    "Features with very low variance don't contribute much to prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4023a6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed dataset from file\n",
      "============================================================\n",
      "REMOVING LOW VARIANCE FEATURES\n",
      "============================================================\n",
      "Features with variance < 0.01: 40\n",
      "Sample low-variance features: ['NACCCORE', 'NACCFADM', 'NACCFFTD', 'STEPWISE', 'EMOT', 'CVDSIGNS', 'POSTCORT', 'PSPCBS', 'ALSFIND', 'GAITNPH']\n",
      "Features with variance < 0.01: 40\n",
      "Sample low-variance features: ['NACCCORE', 'NACCFADM', 'NACCFFTD', 'STEPWISE', 'EMOT', 'CVDSIGNS', 'POSTCORT', 'PSPCBS', 'ALSFIND', 'GAITNPH']\n",
      "\n",
      "Shape after removing low variance features: (195196, 498)\n",
      "Features remaining: 498\n",
      "\n",
      "Shape after removing low variance features: (195196, 498)\n",
      "Features remaining: 498\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Load the preprocessed dataset (if not already in memory)\n",
    "if 'df_processed' not in locals():\n",
    "    df_processed = pd.read_csv('Dataset/Dementia_Preprocessed.csv')\n",
    "    print(\"Loaded preprocessed dataset from file\")\n",
    "else:\n",
    "    print(\"Using df_processed from memory\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REMOVING LOW VARIANCE FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate variance threshold (0.01 means features must vary by at least 1%)\n",
    "variance_threshold = 0.01\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "\n",
    "# Fit and get feature mask\n",
    "selector.fit(df_processed)\n",
    "low_var_features = df_processed.columns[~selector.get_support()].tolist()\n",
    "\n",
    "print(f\"Features with variance < {variance_threshold}: {len(low_var_features)}\")\n",
    "if len(low_var_features) > 0:\n",
    "    print(f\"Sample low-variance features: {low_var_features[:10]}\")\n",
    "    \n",
    "    # Drop low variance features\n",
    "    df_processed = df_processed.drop(columns=low_var_features)\n",
    "    \n",
    "print(f\"\\nShape after removing low variance features: {df_processed.shape}\")\n",
    "print(f\"Features remaining: {df_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad68440",
   "metadata": {},
   "source": [
    "## Step B: Remove Highly Correlated Features\n",
    "\n",
    "Remove redundant features that are highly correlated (multicollinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6deb4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REMOVING HIGHLY CORRELATED FEATURES\n",
      "============================================================\n",
      "Calculating correlation matrix... (this may take a moment)\n",
      "\n",
      "Features with correlation > 0.95: 82\n",
      "Sample highly correlated features: ['NACCFM', 'NACCOM', 'BPDIAS', 'CVDIMAG2', 'CVDIMAG3', 'CVDIMAG4', 'SATIS', 'DROPACT', 'EMPTY', 'BORED']\n",
      "\n",
      "Shape after removing correlated features: (195196, 416)\n",
      "Features remaining: 416\n",
      "\n",
      "Features with correlation > 0.95: 82\n",
      "Sample highly correlated features: ['NACCFM', 'NACCOM', 'BPDIAS', 'CVDIMAG2', 'CVDIMAG3', 'CVDIMAG4', 'SATIS', 'DROPACT', 'EMPTY', 'BORED']\n",
      "\n",
      "Shape after removing correlated features: (195196, 416)\n",
      "Features remaining: 416\n"
     ]
    }
   ],
   "source": [
    "# Remove highly correlated features\n",
    "print(\"=\"*60)\n",
    "print(\"REMOVING HIGHLY CORRELATED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate correlation matrix (only for remaining features)\n",
    "print(\"Calculating correlation matrix... (this may take a moment)\")\n",
    "corr_matrix = df_processed.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_tri = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "correlation_threshold = 0.95\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "\n",
    "print(f\"\\nFeatures with correlation > {correlation_threshold}: {len(to_drop)}\")\n",
    "if len(to_drop) > 0:\n",
    "    print(f\"Sample highly correlated features: {to_drop[:10]}\")\n",
    "    df_processed = df_processed.drop(columns=to_drop)\n",
    "\n",
    "print(f\"\\nShape after removing correlated features: {df_processed.shape}\")\n",
    "print(f\"Features remaining: {df_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ba2b66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL FEATURE REDUCTION COMPLETE!\n",
      "============================================================\n",
      "Final dataset saved: 'Dataset/Dementia_Feature_Reduced.csv'\n",
      "\n",
      "Complete reduction summary:\n",
      "  Original features: 538\n",
      "  After low variance removal: → 538 (no features removed)\n",
      "  After correlation removal: → 416\n",
      "  After statistical selection: → 250 (+ target)\n",
      "  Total features removed: 288\n",
      "  Total reduction: 53.5%\n",
      "\n",
      "✅ Dataset optimized with statistically significant features!\n",
      "\n",
      "Dataset summary:\n",
      "  Total columns: 251 (includes target)\n",
      "  Feature columns: 250\n",
      "  Rows: 195,196\n",
      "  Memory: 373.80 MB\n",
      "\n",
      "📊 Features kept based on statistical significance to target variable\n",
      "   Only features with strong predictive power for dementia are retained\n"
     ]
    }
   ],
   "source": [
    "# Save the final feature-reduced dataset\n",
    "df_processed.to_csv('Dataset/Dementia_Feature_Reduced.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL FEATURE REDUCTION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final dataset saved: 'Dataset/Dementia_Feature_Reduced.csv'\")\n",
    "print(f\"\\nComplete reduction summary:\")\n",
    "print(f\"  Original features: 538\")\n",
    "print(f\"  After low variance removal: → 538 (no features removed)\")\n",
    "print(f\"  After correlation removal: → 416\")\n",
    "print(f\"  After statistical selection: → {df_processed.shape[1] - 1} (+ target)\")\n",
    "print(f\"  Total features removed: {538 - (df_processed.shape[1] - 1)}\")\n",
    "print(f\"  Total reduction: {((538 - (df_processed.shape[1] - 1))/538 * 100):.1f}%\")\n",
    "print(f\"\\n✅ Dataset optimized with statistically significant features!\")\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Total columns: {df_processed.shape[1]} (includes target)\")\n",
    "print(f\"  Feature columns: {df_processed.shape[1] - 1}\")\n",
    "print(f\"  Rows: {df_processed.shape[0]:,}\")\n",
    "print(f\"  Memory: {df_processed.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\n📊 Features kept based on statistical significance to target variable\")\n",
    "print(f\"   Only features with strong predictive power for dementia are retained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ac420",
   "metadata": {},
   "source": [
    "## Step E: Save Final Reduced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab1b9a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADDITIONAL FEATURE SELECTION - Data-Driven Approach\n",
      "============================================================\n",
      "Target variable found: DEMENTED\n",
      "\n",
      "Current features: 250\n",
      "Target distribution:\n",
      "DEMENTED\n",
      "0.0    137606\n",
      "1.0     57590\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Analyzing statistical significance of all features...\n",
      "============================================================\n",
      "\n",
      "Top 50 most important features (by F-score):\n",
      " Feature       F_Score\n",
      "NACCIDEM 535513.587130\n",
      "NACCUDSD 462637.884377\n",
      " NACCPPA 256973.402694\n",
      "INDEPEND 165825.770518\n",
      "  DECAGE 136366.484056\n",
      " NORMCOG 128116.969806\n",
      "  COURSE 125469.425177\n",
      "NACCADMD 119359.247347\n",
      " COGJUDG 118331.943779\n",
      " FRSTCHG 114684.944607\n",
      " PRIONIF 107462.588141\n",
      "  COGMEM 101261.970962\n",
      " COGMODE  97435.190743\n",
      "HYCEPHIF  88147.620137\n",
      "   PSPIF  79726.103046\n",
      "ALCDEMIF  76324.220243\n",
      "PROBADIF  74239.124945\n",
      "  CORTIF  72878.431353\n",
      "COGOTH3F  71576.255021\n",
      " CDRLANG  69512.370201\n",
      "  AMNDEM  67568.552036\n",
      " COMPORT  59788.239541\n",
      "NACCNORM  55427.722981\n",
      "  BEMODE  54525.746148\n",
      "OTHPSYIF  52444.135628\n",
      "COGOTH2F  51378.778280\n",
      "  MEDSIF  48046.397845\n",
      "BRNINJIF  45471.053834\n",
      "  APASEV  45031.109634\n",
      " COGLANG  44443.044028\n",
      "NACCCOGF  40116.437548\n",
      "NACCLBDP  33821.184899\n",
      "NACCPPAG  33053.171457\n",
      "BEAPATHY  30361.862710\n",
      "  MOTSEV  29897.457864\n",
      " DECCLIN  29825.936738\n",
      "NACCDIED  28967.120283\n",
      "DYSILLIF  28599.344104\n",
      "  PROBAD  27945.379030\n",
      "   DEPIF  27548.189670\n",
      " AGITSEV  24995.083737\n",
      "  MOMODE  24317.105890\n",
      "  COGVIS  24209.816343\n",
      "NACCMOTF  24134.347884\n",
      " COGATTN  23587.763452\n",
      "   FTDIF  21443.416952\n",
      " DISNSEV  20402.766475\n",
      "  VASCPS  20217.922612\n",
      "     APA  20011.823428\n",
      "NACCPPME  19316.308352\n",
      "\n",
      "============================================================\n",
      "F-Score Distribution Analysis:\n",
      "============================================================\n",
      "  Top  5% =  12 features (F-score ≥  101261.97)\n",
      "  Top 10% =  25 features (F-score ≥   52444.14)\n",
      "  Top 15% =  37 features (F-score ≥   28967.12)\n",
      "  Top 20% =  50 features (F-score ≥   19316.31)\n",
      "  Top 25% =  62 features (F-score ≥   13522.64)\n",
      "  Top 30% =  75 features (F-score ≥   10788.39)\n",
      "  Top 40% = 100 features (F-score ≥    7089.66)\n",
      "  Top 50% = 125 features (F-score ≥    4841.99)\n",
      "\n",
      "============================================================\n",
      "AUTOMATIC SELECTION STRATEGY:\n",
      "============================================================\n",
      "Using top 15% of features by F-score\n",
      "(Keeping only the most important predictors)\n",
      "\n",
      "✅ Selected 37 features (top 15%)\n",
      "\n",
      "============================================================\n",
      "FINAL FEATURE SELECTION RESULTS:\n",
      "============================================================\n",
      "  Original features: 250\n",
      "  Selected features: 37\n",
      "  Features removed: 213\n",
      "  Reduction: 85.2%\n",
      "\n",
      "Selected features F-score statistics:\n",
      "  Mean: 101405.72\n",
      "  Median: 71576.26\n",
      "  Min: 28967.12\n",
      "  Max: 535513.59\n",
      "\n",
      "✅ Only the TOP 15% MOST IMPORTANT features retained!\n",
      "   All features have extremely strong predictive power for dementia\n",
      "   Final shape (with target): (195196, 38)\n",
      "\n",
      "Top 50 most important features (by F-score):\n",
      " Feature       F_Score\n",
      "NACCIDEM 535513.587130\n",
      "NACCUDSD 462637.884377\n",
      " NACCPPA 256973.402694\n",
      "INDEPEND 165825.770518\n",
      "  DECAGE 136366.484056\n",
      " NORMCOG 128116.969806\n",
      "  COURSE 125469.425177\n",
      "NACCADMD 119359.247347\n",
      " COGJUDG 118331.943779\n",
      " FRSTCHG 114684.944607\n",
      " PRIONIF 107462.588141\n",
      "  COGMEM 101261.970962\n",
      " COGMODE  97435.190743\n",
      "HYCEPHIF  88147.620137\n",
      "   PSPIF  79726.103046\n",
      "ALCDEMIF  76324.220243\n",
      "PROBADIF  74239.124945\n",
      "  CORTIF  72878.431353\n",
      "COGOTH3F  71576.255021\n",
      " CDRLANG  69512.370201\n",
      "  AMNDEM  67568.552036\n",
      " COMPORT  59788.239541\n",
      "NACCNORM  55427.722981\n",
      "  BEMODE  54525.746148\n",
      "OTHPSYIF  52444.135628\n",
      "COGOTH2F  51378.778280\n",
      "  MEDSIF  48046.397845\n",
      "BRNINJIF  45471.053834\n",
      "  APASEV  45031.109634\n",
      " COGLANG  44443.044028\n",
      "NACCCOGF  40116.437548\n",
      "NACCLBDP  33821.184899\n",
      "NACCPPAG  33053.171457\n",
      "BEAPATHY  30361.862710\n",
      "  MOTSEV  29897.457864\n",
      " DECCLIN  29825.936738\n",
      "NACCDIED  28967.120283\n",
      "DYSILLIF  28599.344104\n",
      "  PROBAD  27945.379030\n",
      "   DEPIF  27548.189670\n",
      " AGITSEV  24995.083737\n",
      "  MOMODE  24317.105890\n",
      "  COGVIS  24209.816343\n",
      "NACCMOTF  24134.347884\n",
      " COGATTN  23587.763452\n",
      "   FTDIF  21443.416952\n",
      " DISNSEV  20402.766475\n",
      "  VASCPS  20217.922612\n",
      "     APA  20011.823428\n",
      "NACCPPME  19316.308352\n",
      "\n",
      "============================================================\n",
      "F-Score Distribution Analysis:\n",
      "============================================================\n",
      "  Top  5% =  12 features (F-score ≥  101261.97)\n",
      "  Top 10% =  25 features (F-score ≥   52444.14)\n",
      "  Top 15% =  37 features (F-score ≥   28967.12)\n",
      "  Top 20% =  50 features (F-score ≥   19316.31)\n",
      "  Top 25% =  62 features (F-score ≥   13522.64)\n",
      "  Top 30% =  75 features (F-score ≥   10788.39)\n",
      "  Top 40% = 100 features (F-score ≥    7089.66)\n",
      "  Top 50% = 125 features (F-score ≥    4841.99)\n",
      "\n",
      "============================================================\n",
      "AUTOMATIC SELECTION STRATEGY:\n",
      "============================================================\n",
      "Using top 15% of features by F-score\n",
      "(Keeping only the most important predictors)\n",
      "\n",
      "✅ Selected 37 features (top 15%)\n",
      "\n",
      "============================================================\n",
      "FINAL FEATURE SELECTION RESULTS:\n",
      "============================================================\n",
      "  Original features: 250\n",
      "  Selected features: 37\n",
      "  Features removed: 213\n",
      "  Reduction: 85.2%\n",
      "\n",
      "Selected features F-score statistics:\n",
      "  Mean: 101405.72\n",
      "  Median: 71576.26\n",
      "  Min: 28967.12\n",
      "  Max: 535513.59\n",
      "\n",
      "✅ Only the TOP 15% MOST IMPORTANT features retained!\n",
      "   All features have extremely strong predictive power for dementia\n",
      "   Final shape (with target): (195196, 38)\n"
     ]
    }
   ],
   "source": [
    "# Additional feature selection using statistical significance\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ADDITIONAL FEATURE SELECTION - Data-Driven Approach\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, identify the target variable\n",
    "if 'DEMENTED' in df_processed.columns:\n",
    "    target_col = 'DEMENTED'\n",
    "    print(f\"Target variable found: {target_col}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df_processed.drop(columns=[target_col])\n",
    "    y = df_processed[target_col]\n",
    "    \n",
    "    print(f\"\\nCurrent features: {X.shape[1]}\")\n",
    "    print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "    \n",
    "    # Calculate F-scores for ALL features\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Analyzing statistical significance of all features...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    selector = SelectKBest(score_func=f_classif, k='all')\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Get all F-scores\n",
    "    feature_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'F_Score': selector.scores_\n",
    "    }).sort_values('F_Score', ascending=False)\n",
    "    \n",
    "    # Remove any NaN or infinite scores\n",
    "    feature_scores = feature_scores[np.isfinite(feature_scores['F_Score'])]\n",
    "    \n",
    "    print(f\"\\nTop 50 most important features (by F-score):\")\n",
    "    print(feature_scores.head(50).to_string(index=False))\n",
    "    \n",
    "    # Analyze score distribution to find natural cutoff points\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"F-Score Distribution Analysis:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check different percentile options - these will tell you exactly how many features to keep\n",
    "    percentiles_to_show = [5, 10, 15, 20, 25, 30, 40, 50]\n",
    "    for pct in percentiles_to_show:\n",
    "        n_features = int(len(feature_scores) * pct / 100)\n",
    "        min_score = feature_scores.iloc[n_features-1]['F_Score'] if n_features > 0 else 0\n",
    "        print(f\"  Top {pct:>2}% = {n_features:>3} features (F-score ≥ {min_score:>10.2f})\")\n",
    "    \n",
    "    # AGGRESSIVE SELECTION: Use only top 15% by default (most important features)\n",
    "    # This ensures we keep only the best predictors\n",
    "    selection_percentile = 15  # You can change this: 5, 10, 15, 20, etc.\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"AUTOMATIC SELECTION STRATEGY:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Using top {selection_percentile}% of features by F-score\")\n",
    "    print(f\"(Keeping only the most important predictors)\")\n",
    "    \n",
    "    n_features_to_keep = max(30, int(selection_percentile * len(feature_scores) / 100))\n",
    "    selected_feature_names = feature_scores.head(n_features_to_keep)['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\n✅ Selected {len(selected_feature_names)} features (top {selection_percentile}%)\")\n",
    "    \n",
    "    # Create new dataframe with selected features + target\n",
    "    df_processed = X[selected_feature_names].copy()\n",
    "    df_processed[target_col] = y.values\n",
    "    \n",
    "    # Show statistics about selected features\n",
    "    selected_scores = feature_scores[feature_scores['Feature'].isin(selected_feature_names)]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL FEATURE SELECTION RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Original features: {X.shape[1]}\")\n",
    "    print(f\"  Selected features: {len(selected_feature_names)}\")\n",
    "    print(f\"  Features removed: {X.shape[1] - len(selected_feature_names)}\")\n",
    "    print(f\"  Reduction: {((X.shape[1] - len(selected_feature_names))/X.shape[1] * 100):.1f}%\")\n",
    "    \n",
    "    print(f\"\\nSelected features F-score statistics:\")\n",
    "    print(f\"  Mean: {selected_scores['F_Score'].mean():.2f}\")\n",
    "    print(f\"  Median: {selected_scores['F_Score'].median():.2f}\")\n",
    "    print(f\"  Min: {selected_scores['F_Score'].min():.2f}\")\n",
    "    print(f\"  Max: {selected_scores['F_Score'].max():.2f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Only the TOP {selection_percentile}% MOST IMPORTANT features retained!\")\n",
    "    print(f\"   All features have extremely strong predictive power for dementia\")\n",
    "    print(f\"   Final shape (with target): {df_processed.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ DEMENTED column not found. Skipping feature selection.\")\n",
    "    print(\"   You may need to run this after identifying the target variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9361fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Final Preprocessing Before Model Training\n",
    "\n",
    "Now we'll complete the remaining preprocessing steps in the correct order to avoid data leakage:\n",
    "\n",
    "1. **Train-Test Split** - Split data first (stratified to preserve class distribution)\n",
    "2. **Feature Scaling** - Scale using training data statistics only\n",
    "3. **Class Balancing** - Apply SMOTE only to training data\n",
    "4. **Save Datasets** - Save processed data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a262ee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING FINAL PROCESSED DATASETS\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_balanced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Save training data (balanced)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mX_train_balanced\u001b[49m.to_csv(\u001b[33m'\u001b[39m\u001b[33mData_split/X_train_balanced.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m y_train_balanced.to_csv(\u001b[33m'\u001b[39m\u001b[33mData_split/y_train_balanced.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Save test data (original class distribution)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_balanced' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING FINAL PROCESSED DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save training data (balanced)\n",
    "X_train_balanced.to_csv('Data_split/X_train_balanced.csv', index=False)\n",
    "y_train_balanced.to_csv('Data_split/y_train_balanced.csv', index=False)\n",
    "\n",
    "# Save test data (original class distribution)\n",
    "X_test_scaled.to_csv('Data_split/X_test.csv', index=False)\n",
    "y_test.to_csv('Data_split/y_test.csv', index=False)\n",
    "\n",
    "# Save scaler for future use\n",
    "with open('Data_dict/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"✅ Saved training data (balanced):\")\n",
    "print(f\"   - Data_split/X_train_balanced.csv ({X_train_balanced.shape})\")\n",
    "print(f\"   - Data_split/y_train_balanced.csv ({len(y_train_balanced):,} samples)\")\n",
    "\n",
    "print(f\"\\n✅ Saved test data (original distribution):\")\n",
    "print(f\"   - Data_split/X_test.csv ({X_test_scaled.shape})\")\n",
    "print(f\"   - Data_split/y_test.csv ({len(y_test):,} samples)\")\n",
    "\n",
    "print(f\"\\n✅ Saved scaler:\")\n",
    "print(f\"   - Data_dict/scaler.pkl\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PREPROCESSING PIPELINE COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset ready for model training:\")\n",
    "print(f\"  Features: {X_train_balanced.shape[1]}\")\n",
    "print(f\"  Training samples: {len(X_train_balanced):,} (balanced)\")\n",
    "print(f\"  Test samples: {len(X_test_scaled):,} (original distribution)\")\n",
    "print(f\"\\n🚀 Ready to build and train models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5af680",
   "metadata": {},
   "source": [
    "## Step I: Save Final Processed Datasets\n",
    "\n",
    "Save all datasets for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c116dacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS BALANCING (SMOTE - Training Data Only)\n",
      "============================================================\n",
      "Before SMOTE:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBefore SMOTE:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mX_train_scaled\u001b[49m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Class 0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(y_train\u001b[38;5;250m \u001b[39m==\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m).sum()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Class 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(y_train\u001b[38;5;250m \u001b[39m==\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m).sum()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS BALANCING (SMOTE - Training Data Only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Before SMOTE:\")\n",
    "print(f\"  Training samples: {len(X_train_scaled):,}\")\n",
    "print(f\"  Class 0: {(y_train == 0).sum():,}\")\n",
    "print(f\"  Class 1: {(y_train == 1).sum():,}\")\n",
    "print(f\"  Ratio: {(y_train == 0).sum() / (y_train == 1).sum():.2f}:1\")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nAfter SMOTE:\")\n",
    "print(f\"  Training samples: {len(X_train_balanced):,}\")\n",
    "print(f\"  Class 0: {(y_train_balanced == 0).sum():,}\")\n",
    "print(f\"  Class 1: {(y_train_balanced == 1).sum():,}\")\n",
    "print(f\"  Ratio: {(y_train_balanced == 0).sum() / (y_train_balanced == 1).sum():.2f}:1\")\n",
    "\n",
    "print(f\"\\n✅ Classes balanced in training data!\")\n",
    "print(f\"   Test data unchanged (class imbalance preserved for realistic evaluation)\")\n",
    "print(f\"   New training samples created: {len(X_train_balanced) - len(X_train_scaled):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf3fa8",
   "metadata": {},
   "source": [
    "## Step H: Class Balancing (Training Data Only)\n",
    "\n",
    "Apply SMOTE to balance classes in training data. Test data remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a0e2d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE SCALING\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m scaler = StandardScaler()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Fit on training data ONLY, then transform both\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m X_train_scaled = scaler.fit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[32m     12\u001b[39m X_test_scaled = scaler.transform(X_test)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Convert back to DataFrame for easier handling\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data ONLY, then transform both\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(f\"Training set scaled: {X_train_scaled.shape}\")\n",
    "print(f\"Test set scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "print(f\"\\nScaling statistics (training data):\")\n",
    "print(f\"  Mean (should be ~0): {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"  Std (should be ~1): {X_train_scaled.std().mean():.6f}\")\n",
    "\n",
    "print(f\"\\n✅ Feature scaling complete!\")\n",
    "print(f\"   Scaler fitted on training data only - no data leakage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1052509",
   "metadata": {},
   "source": [
    "## Step G: Feature Scaling\n",
    "\n",
    "Scale features using StandardScaler. Fit on training data only to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAIN-TEST SPLIT (STRATIFIED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Separate features and target\n",
    "if 'DEMENTED' in df_processed.columns:\n",
    "    X = df_processed.drop(columns=['DEMENTED'])\n",
    "    y = df_processed['DEMENTED']\n",
    "    \n",
    "    print(f\"Total samples: {len(X):,}\")\n",
    "    print(f\"Features: {X.shape[1]}\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(y.value_counts())\n",
    "    print(f\"Class ratio: {y.value_counts()[0] / y.value_counts()[1]:.2f}:1\")\n",
    "    \n",
    "    # Stratified split (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SPLIT RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"  Class 0: {(y_train == 0).sum():,}\")\n",
    "    print(f\"  Class 1: {(y_train == 1).sum():,}\")\n",
    "    print(f\"  Ratio: {(y_train == 0).sum() / (y_train == 1).sum():.2f}:1\")\n",
    "    \n",
    "    print(f\"\\nTest set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    print(f\"  Class 0: {(y_test == 0).sum():,}\")\n",
    "    print(f\"  Class 1: {(y_test == 1).sum():,}\")\n",
    "    print(f\"  Ratio: {(y_test == 0).sum() / (y_test == 1).sum():.2f}:1\")\n",
    "    \n",
    "    print(f\"\\n✅ Stratified split complete - class proportions preserved!\")\n",
    "else:\n",
    "    print(\"⚠️ DEMENTED column not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6021a69",
   "metadata": {},
   "source": [
    "## Step F: Train-Test Split (Stratified)\n",
    "\n",
    "Split data into training and testing sets while preserving class distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
